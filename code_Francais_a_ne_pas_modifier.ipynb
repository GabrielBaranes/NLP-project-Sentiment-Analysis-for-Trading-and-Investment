{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9871e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\edbar\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for httpx==0.13.3 from https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.1.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for httpcore==0.9.* from https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for h11<0.10,>=0.8 from https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\edbar\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.4\n",
      "    Uninstalling httpcore-1.0.4:\n",
      "      Successfully uninstalled httpcore-1.0.4\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed h11-0.9.0 httpcore-0.9.1 httpx-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "python-telegram-bot 21.0.1 requires httpx~=0.27, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370922cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edbar\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import os\n",
    "from googletrans import Translator\n",
    "from openpyxl.utils import get_column_letter\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import TFBertModel\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    max_chunk_size = 450  # Use a value slightly less than 500 to stay within the limit\n",
    "\n",
    "    # Split the text into chunks of maximum allowed length\n",
    "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "\n",
    "    # Translate each chunk and join the translations\n",
    "    translated_chunks = [translator.translate(chunk, src='fr', dest='en').text for chunk in chunks]\n",
    "    translated_text = \" \".join(translated_chunks)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def get_articles_from_api(api_key, keyword, sources, from_date, to_date):\n",
    "    base_url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": keyword,\n",
    "        \"language\": \"fr\",\n",
    "        \"sources\": \",\".join(sources),\n",
    "        \"from\": from_date.isoformat(),\n",
    "        \"to\": to_date.isoformat(),\n",
    "        \"pageSize\": 100,  # Set a larger page size to get more articles in a single request\n",
    "        \"apiKey\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        return data[\"articles\"]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error during API request:\", e)\n",
    "        return []\n",
    "\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # List of potential tags for article content\n",
    "        possible_tags = [\"p\", \"div\", \"article\", \"section\"]\n",
    "\n",
    "        # Find the first tag that contains text\n",
    "        for tag in possible_tags:\n",
    "            tag_elements = soup.find_all(tag)\n",
    "            if tag_elements:\n",
    "                article_text = \" \".join([element.get_text().strip() for element in tag_elements])\n",
    "                return article_text\n",
    "\n",
    "        # If no text found, return None\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error during article request:\", e)\n",
    "        return None\n",
    "\n",
    "def save_text_to_file(text, file_name):\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "def create_excel_with_links(df, file_path):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # Add column headers\n",
    "    headers = [\"Date\", \"Title\", \"Link\", \"Source\", \"Label\", \"Probability\"]\n",
    "    ws.append(headers)\n",
    "\n",
    "    # Add the DataFrame data to the Excel file\n",
    "    for row in dataframe_to_rows(df, index=False, header=True):\n",
    "        new_row = []\n",
    "        for cell in row:\n",
    "            if isinstance(cell, tf.Tensor):\n",
    "                new_row.append(float(cell.numpy()))\n",
    "            else:\n",
    "                new_row.append(cell)\n",
    "        ws.append(new_row)\n",
    "\n",
    "    # Make the links clickable in the \"Link\" column\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=3, max_col=3):\n",
    "        for cell in row:\n",
    "            cell.hyperlink = cell.value\n",
    "            cell.style = \"Hyperlink\"\n",
    "            \n",
    "        # Adjust column widths\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        column = [cell for cell in column]\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2) * 1.2\n",
    "        ws.column_dimensions[get_column_letter(column[0].column)].width = adjusted_width\n",
    "\n",
    "    # Save the Excel file\n",
    "    wb.save(file_path)\n",
    "\n",
    "    \n",
    "def perform_sentiment_analysis(text):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Make a prediction with the loaded model\n",
    "    outputs = loaded_model([input_ids, token_type_ids, attention_mask])\n",
    "\n",
    "    # Extract the predicted probabilities from the output tensors\n",
    "    probs = outputs[0]\n",
    "\n",
    "    # Convert probabilities to class labels (positive or negative)\n",
    "    class_labels = ['Negative', 'Positive']\n",
    "    predicted_class_index = np.argmax(probs)\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "    # Get the predicted probability for the predicted class\n",
    "    predicted_class_probability = probs[predicted_class_index]\n",
    "\n",
    "    return predicted_class_label, predicted_class_probability\n",
    "\n",
    "def search_articles(keyword, number_articles=5):\n",
    "    # Your NewsAPI API key\n",
    "    api_key = \"4a3afce11f134327a142694aa36649bd\"\n",
    "\n",
    "    # Date range for search (30 days before the current date until the current date)\n",
    "    from_date = date.today() - timedelta(days=30)\n",
    "    to_date = date.today()\n",
    "\n",
    "    # Sources to search for articles\n",
    "    sources_francais = [\n",
    "        'le-monde', 'le-figaro', 'usinenouvelle', 'challenges', 'tradingsat', 'capital', 'liberation', 'le-parisien', '20-minutes',\n",
    "        'bfmtv', 'france24', 'les-echos', 'courrier-international', 'la-croix'\n",
    "    ]\n",
    "\n",
    "    articles = get_articles_from_api(api_key, keyword, sources_francais, from_date, to_date)\n",
    "\n",
    "    # Determine the number of articles to process\n",
    "    num_articles_to_process = min(number_articles, len(articles))\n",
    "\n",
    "    # Extract information from articles, translate to English, and save text to .txt files\n",
    "    #folder_name = keyword\n",
    "    folder_name = f\"{keyword}_French\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    for i, article in enumerate(articles[:num_articles_to_process], 1):\n",
    "        title = article[\"title\"]\n",
    "        url = article[\"url\"]\n",
    "        article_text = get_article_text(url)\n",
    "        if article_text is not None:\n",
    "            translated_text = translate_to_english(article_text)  # Translate to English\n",
    "            file_name = os.path.join(folder_name, f\"article_Francais_{i}.txt\")\n",
    "            save_text_to_file(translated_text, file_name)  # Save the translated text\n",
    "\n",
    "    # Create a DataFrame and save the articles to an Excel file\n",
    "    article_data = []\n",
    "    for article in articles[:num_articles_to_process]:\n",
    "        title = article[\"title\"]\n",
    "        url = article[\"url\"]\n",
    "        source = article[\"source\"][\"name\"]  # Ajout: Récupérer la source de l'article\n",
    "\n",
    "        article_text = get_article_text(url)\n",
    "        if article_text is not None:\n",
    "            translated_text = translate_to_english(article_text)  # Translate to English\n",
    "            label, probability = perform_sentiment_analysis(translated_text)  # Analyse des sentiments\n",
    "\n",
    "            # Ajout: Ajouter les données au DataFrame\n",
    "            article_info = {\n",
    "                \"Date\": article[\"publishedAt\"],\n",
    "                \"Title\": title,\n",
    "                \"Link\": url,\n",
    "                \"Source\": source,\n",
    "                \"Label\": label,\n",
    "                \"Probability\": probability * 100  # Convertir la probabilité en pourcentage\n",
    "            }\n",
    "            article_data.append(article_info)\n",
    "\n",
    "            # Sauvegarder le texte traduit dans un fichier .txt (comme dans le code actuel)\n",
    "            file_name = os.path.join(folder_name, f\"article_francais_{i}.txt\")\n",
    "            save_text_to_file(translated_text, file_name)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(article_data)\n",
    "    df.to_pickle(\"NVIDIA_French.pkl\")\n",
    "    excel_file = f\"{keyword}_articles_Francais.xlsx\"\n",
    "    excel_filepath = os.path.join(folder_name, excel_file)\n",
    "    create_excel_with_links(df, excel_filepath)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Load the BERT model\n",
    "bert_type = 'bert-base-cased'\n",
    "bert = TFBertModel.from_pretrained(bert_type)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "# Load the sentiment analysis model including the TFBertModel object in custom_objects\n",
    "def custom_objects():\n",
    "    return {\"F1Score\": tfa.metrics.F1Score, \"TFBertModel\": TFBertModel}\n",
    "\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('modele_bert.h5', custom_objects=custom_objects())\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033d729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test la fonction avec l'exemple que vous avez fourni\n",
    "keyword = \"NVIDIA\"\n",
    "search_articles(keyword, number_articles=100)  # Fetch and process 10 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0770f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"NVIDIA_French.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef97fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-23T08:25:39Z</td>\n",
       "      <td>« Le déclin d’Intel est la conséquence de troi...</td>\n",
       "      <td>https://www.lemonde.fr/economie/article/2024/0...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>tf.Tensor(75.792984, shape=(), dtype=float32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-13T12:00:12Z</td>\n",
       "      <td>« La géopolitique s’invite avec fracas dans un...</td>\n",
       "      <td>https://www.lemonde.fr/idees/article/2024/09/1...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>tf.Tensor(75.792984, shape=(), dtype=float32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-17T07:15:35Z</td>\n",
       "      <td>Intel repousse ses projets d’usine en Allemagn...</td>\n",
       "      <td>https://www.liberation.fr/economie/economie-nu...</td>\n",
       "      <td>Libération</td>\n",
       "      <td>Negative</td>\n",
       "      <td>tf.Tensor(81.864044, shape=(), dtype=float32)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                              Title  \\\n",
       "0  2024-09-23T08:25:39Z  « Le déclin d’Intel est la conséquence de troi...   \n",
       "1  2024-09-13T12:00:12Z  « La géopolitique s’invite avec fracas dans un...   \n",
       "2  2024-09-17T07:15:35Z  Intel repousse ses projets d’usine en Allemagn...   \n",
       "\n",
       "                                                Link      Source     Label  \\\n",
       "0  https://www.lemonde.fr/economie/article/2024/0...    Le Monde  Negative   \n",
       "1  https://www.lemonde.fr/idees/article/2024/09/1...    Le Monde  Negative   \n",
       "2  https://www.liberation.fr/economie/economie-nu...  Libération  Negative   \n",
       "\n",
       "                                     Probability  \n",
       "0  tf.Tensor(75.792984, shape=(), dtype=float32)  \n",
       "1  tf.Tensor(75.792984, shape=(), dtype=float32)  \n",
       "2  tf.Tensor(81.864044, shape=(), dtype=float32)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cf7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Probability\"] = df[\"Probability\"].apply(lambda x: float(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c579bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-23T08:25:39Z</td>\n",
       "      <td>« Le déclin d’Intel est la conséquence de troi...</td>\n",
       "      <td>https://www.lemonde.fr/economie/article/2024/0...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>75.792984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-13T12:00:12Z</td>\n",
       "      <td>« La géopolitique s’invite avec fracas dans un...</td>\n",
       "      <td>https://www.lemonde.fr/idees/article/2024/09/1...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>75.792984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-17T07:15:35Z</td>\n",
       "      <td>Intel repousse ses projets d’usine en Allemagn...</td>\n",
       "      <td>https://www.liberation.fr/economie/economie-nu...</td>\n",
       "      <td>Libération</td>\n",
       "      <td>Negative</td>\n",
       "      <td>81.864044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                              Title  \\\n",
       "0  2024-09-23T08:25:39Z  « Le déclin d’Intel est la conséquence de troi...   \n",
       "1  2024-09-13T12:00:12Z  « La géopolitique s’invite avec fracas dans un...   \n",
       "2  2024-09-17T07:15:35Z  Intel repousse ses projets d’usine en Allemagn...   \n",
       "\n",
       "                                                Link      Source     Label  \\\n",
       "0  https://www.lemonde.fr/economie/article/2024/0...    Le Monde  Negative   \n",
       "1  https://www.lemonde.fr/idees/article/2024/09/1...    Le Monde  Negative   \n",
       "2  https://www.liberation.fr/economie/economie-nu...  Libération  Negative   \n",
       "\n",
       "   Probability  \n",
       "0    75.792984  \n",
       "1    75.792984  \n",
       "2    81.864044  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2b36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Probability': 'Categorical_Accuracy'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aae7587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "      <th>Categorical_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-23T08:25:39Z</td>\n",
       "      <td>« Le déclin d’Intel est la conséquence de troi...</td>\n",
       "      <td>https://www.lemonde.fr/economie/article/2024/0...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>75.792984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-13T12:00:12Z</td>\n",
       "      <td>« La géopolitique s’invite avec fracas dans un...</td>\n",
       "      <td>https://www.lemonde.fr/idees/article/2024/09/1...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>Negative</td>\n",
       "      <td>75.792984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-17T07:15:35Z</td>\n",
       "      <td>Intel repousse ses projets d’usine en Allemagn...</td>\n",
       "      <td>https://www.liberation.fr/economie/economie-nu...</td>\n",
       "      <td>Libération</td>\n",
       "      <td>Negative</td>\n",
       "      <td>81.864044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                              Title  \\\n",
       "0  2024-09-23T08:25:39Z  « Le déclin d’Intel est la conséquence de troi...   \n",
       "1  2024-09-13T12:00:12Z  « La géopolitique s’invite avec fracas dans un...   \n",
       "2  2024-09-17T07:15:35Z  Intel repousse ses projets d’usine en Allemagn...   \n",
       "\n",
       "                                                Link      Source     Label  \\\n",
       "0  https://www.lemonde.fr/economie/article/2024/0...    Le Monde  Negative   \n",
       "1  https://www.lemonde.fr/idees/article/2024/09/1...    Le Monde  Negative   \n",
       "2  https://www.liberation.fr/economie/economie-nu...  Libération  Negative   \n",
       "\n",
       "   Categorical_Accuracy  \n",
       "0             75.792984  \n",
       "1             75.792984  \n",
       "2             81.864044  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8c2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"Final_NVIDIA_French.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f64725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
